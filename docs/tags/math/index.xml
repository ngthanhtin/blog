<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>math on Tin&#39;s Blog</title>
    <link>https://ngthanhtin.github.io/blog/tags/math/</link>
    <description>Recent content in math on Tin&#39;s Blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <lastBuildDate>Sun, 14 Mar 2021 00:00:00 +0000</lastBuildDate><atom:link href="https://ngthanhtin.github.io/blog/tags/math/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Giới thiệu về Optmization</title>
      <link>https://ngthanhtin.github.io/blog/optimization-gioi-thieu/</link>
      <pubDate>Sun, 14 Mar 2021 00:00:00 +0000</pubDate>
      
      <guid>https://ngthanhtin.github.io/blog/optimization-gioi-thieu/</guid>
      <description>Hi, xin chào các bạn, hôm này mình sẽ giới thiệu cho các bạn về toán Optmization. Nói về Deep Learning thì không thể thiếu được Optimization, chắc hẳn ai làm DL đều đã xài qua những cái như SGD, Adam, Adagrad, KAFC, etc. Tác dụng của optimization đó là giúp tìm ra local minima của model, khi đó ta có thể nói model đã converge.
Tuy nhiên, để hiểu rõ cách thức hoạt động của optimization, chúng ta phải hiểu nó là gì, và nó áp dụng như thế nào?</description>
    </item>
    
  </channel>
</rss>
