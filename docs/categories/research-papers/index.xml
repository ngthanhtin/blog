<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Research Papers on Tin&#39;s Blog</title>
    <link>https://ngthanhtin.github.io/blog/categories/research-papers/</link>
    <description>Recent content in Research Papers on Tin&#39;s Blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <lastBuildDate>Wed, 05 Jul 2023 00:00:00 +0000</lastBuildDate><atom:link href="https://ngthanhtin.github.io/blog/categories/research-papers/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Train a MaskFormer Segmentation Model with Hugging Face Transformers</title>
      <link>https://ngthanhtin.github.io/blog/research_papers/2023-06-05-mask2former-training/</link>
      <pubDate>Wed, 05 Jul 2023 00:00:00 +0000</pubDate>
      
      <guid>https://ngthanhtin.github.io/blog/research_papers/2023-06-05-mask2former-training/</guid>
      <description>Hello everyone, Link paper: MaskFormer
1. Introduction Đầu tiên, chúng ta cần hiểu VQA là gì, nếu bạn chưa biết thì có thể xem bài viết trước của mình để hiểu nó, link. Nguyên lý của bài toán này đó là làm sao có thể kết hợp 2 feature vision và text lại với nhau thành một vector chung, sau đó đưa vector này sang một classifier để tìm ra answer, và có nhiều phương pháp hiện nay sử dụng để kết hợp 2 feature đó lại với nhau.</description>
    </item>
    
    <item>
      <title>Bilinear (Trilinear) Attention</title>
      <link>https://ngthanhtin.github.io/blog/research_papers/2021-02-02-bi-tri-attention/</link>
      <pubDate>Mon, 29 Mar 2021 00:00:00 +0000</pubDate>
      
      <guid>https://ngthanhtin.github.io/blog/research_papers/2021-02-02-bi-tri-attention/</guid>
      <description>Hi các bạn, ở bài post này mình tiếp tục chia sẽ một kĩ thuật Attention dùng trong bài toán Visual Question Answering (VQA) đó là Bi-Attention (Tri-Attention), ở bài trước mình có đề cập tới một kĩ thuật khác gọi là Stacked Attention Link paper: Bilinear Attention Link code: Code
1. Introduction Đầu tiên, chúng ta cần hiểu VQA là gì, nếu bạn chưa biết thì có thể xem bài viết trước của mình để hiểu nó, link.</description>
    </item>
    
    <item>
      <title>Stacked Attention Network</title>
      <link>https://ngthanhtin.github.io/blog/research_papers/2021-02-01-stacked-attention-network/</link>
      <pubDate>Fri, 05 Feb 2021 00:00:00 +0000</pubDate>
      
      <guid>https://ngthanhtin.github.io/blog/research_papers/2021-02-01-stacked-attention-network/</guid>
      <description>Hi, xin chào các bạn, hôm này mình sẽ giới thiệu cho các bạn mô hình Stacked Attention được dùng trong bài toán Visual Question Answering (VQA) Link paper: Stacked Attention Networks for Image Question Answering (2016) Link code: Code
Nội dung chính sẽ bao gồm các phần sau: 1. Giới thiệu bài toàn Visual Question Answering
2. Nguyên lý
3. Phương pháp
4. Giải thuật
5. Ứng dụng
6. Tham khảo</description>
    </item>
    
  </channel>
</rss>
