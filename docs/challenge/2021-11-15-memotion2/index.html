<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <meta http-equiv="X-UA-Compatible" content="ie=edge" />
    <title>Understanding Sentiment, Emotion and its Intensity by Analysing Meme Images | Tin&#39;s Blog</title>

    



<meta name="author" content="Nguyen Thanh Tin" />
<meta name="description" content="" />



<meta name="generator" content="Hugo 0.80.0" />

<link rel="canonical" href="https://ngthanhtin.github.io/blog/challenge/2021-11-15-memotion2/" />


<meta property="og:title" content="Understanding Sentiment, Emotion and its Intensity by Analysing Meme Images" />
<meta property="og:description" content="In this post, I want to introduce a new topic which is took place at First Workshop on ​Multimodal Fact-Checking and Hate Speech Detection in AAAI 2022. And I got the 1st position on the leaderboard of this challenge. Link to the challenge: https://aiisc.ai/defactify/memotion_2.html
1. Abstract In this post, I want to talk about Meme Analysis which is about explaining its Sentiment, Emotion and Intensity.  The growing ubiquity of Internet memes on social media platforms such as Facebook, Instagram, and Twitter further suggests that we can not ignore such multimodal content (vision and language) anymore." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://ngthanhtin.github.io/blog/challenge/2021-11-15-memotion2/" />
<meta property="article:published_time" content="2021-11-15T00:00:00+00:00" />
<meta property="article:modified_time" content="2021-11-15T00:00:00+00:00" />



<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Understanding Sentiment, Emotion and its Intensity by Analysing Meme Images"/>
<meta name="twitter:description" content="In this post, I want to introduce a new topic which is took place at First Workshop on ​Multimodal Fact-Checking and Hate Speech Detection in AAAI 2022. And I got the 1st position on the leaderboard of this challenge. Link to the challenge: https://aiisc.ai/defactify/memotion_2.html
1. Abstract In this post, I want to talk about Meme Analysis which is about explaining its Sentiment, Emotion and Intensity.  The growing ubiquity of Internet memes on social media platforms such as Facebook, Instagram, and Twitter further suggests that we can not ignore such multimodal content (vision and language) anymore."/>

<link rel="stylesheet" href="/blog/css/font-awesome.min.css" />
<link rel="stylesheet" href="/blog/css/semantic.min.css" />
<link rel="stylesheet" href="/blog/css/icomoon.css" />
<link rel="stylesheet" href="/blog/css/OverlayScrollbars.min.css" />
<link rel="stylesheet" href="/blog/css/github-markdown.css" />
<link rel="stylesheet" href="/blog/css/site.css" />

<style>
  a:not(.ui.button):hover {
    text-decoration: underline;
  }

  
  a:not(.ui.button) {
    color: seagreen !important;
  }
  

  

  
  body.default {
    
    background-color: #f7f7f7;
    

    
  }
  

  
  body.dark {
    
    background-color: black;
    

    
  }
  
</style>


    







    
  </head>

  <body class="default">
    
<nav class="ui secondary menu dream-menu">

  <div class="item">
    <i class="large link bullseye icon dream-flip-toggle" title="Flip it!"></i>
  </div>
  <div class="item">
    <i class="large link home icon" title="Blog" onclick="window.location.href = 'https:\/\/ngthanhtin.github.io\/blog\/'"></i>
  </div>
  <div class="item">
    <a href="https://ngthanhtin.github.io/"><i class="fa fa-flask" title="ngthanhtin.github.io"></i></a>
  </div>
  
  
  <div class="item">
    <i class="large link icon theme-switch" onclick="themeSwitch()"></i>
  </div>
  
  
</nav>

    <div class="flip-container">
      <div class="flipper">
        <section class="front">
          <div class="dream-max-width">
            
<div class="ui relaxed grid dream-grid">
  <aside class="sixteen wide mobile sixteen wide tablet four wide computer column dream-single-aside">
    
    
    <div class="ui segment toc">
      <nav id="TableOfContents">
  <ul>
    <li>
      <ul>
        <li><a href="#1-abstract">1. Abstract</a></li>
        <li><a href="#2-model">2. Model</a></li>
        <li><a href="#3-techniques">3. Techniques</a></li>
        <li><a href="#4-result">4. Result</a></li>
        <li><a href="#5-application">5. Application</a></li>
        <li><a href="#6-code-and-paper">6. Code and Paper</a></li>
      </ul>
    </li>
  </ul>
</nav>
    </div>
    
    <div class="ui segment actions">
      <button
        class="ui circular icon button save-as-image"
        title="Save as image"
        onclick="savePostAsImg()">
        <i class="save icon"></i>
      </button>

      <a href="https://twitter.com/intent/tweet?text=Understanding%20Sentiment%2c%20Emotion%20and%20its%20Intensity%20by%20Analysing%20Meme%20Images&url=https%3a%2f%2fngthanhtin.github.io%2fblog%2fchallenge%2f2021-11-15-memotion2%2f" class="ui circular twitter icon button">
  <i class="twitter icon"></i>
</a>
<a href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fngthanhtin.github.io%2fblog%2fchallenge%2f2021-11-15-memotion2%2f" class="ui circular facebook icon button">
  <i class="facebook icon"></i>
</a>

    </div>
  </aside>
  <div class="sixteen wide mobile sixteen wide tablet twelve wide computer column markdown-body dream-single" id="dream-save-post-as-img">
    
    <section class="ui top attached segment">
      <header>
        <h1 class="ui large header">Understanding Sentiment, Emotion and its Intensity by Analysing Meme Images<div class="sub header">
            @
            
              Nguyen Thanh Tin
            

            | 
                Monday, Nov 15, 2021
              

            | 5 minutes read

            | Update at
              
                Monday, Nov 15, 2021
              
          </div>
        </h1>
      </header>

      <article class="main"><p>In this post, I want to introduce a new topic which is took place at First Workshop on ​Multimodal Fact-Checking and Hate Speech Detection in AAAI 2022. And I got the 1st position on the leaderboard of this challenge. Link to the challenge: <a href="https://aiisc.ai/defactify/memotion_2.html">https://aiisc.ai/defactify/memotion_2.html</a></p>
<h3 id="1-abstract">1. Abstract</h3>
<p>In this post, I want to talk about Meme Analysis which is about explaining its Sentiment, Emotion and Intensity. <br/>
The growing ubiquity of Internet memes on social media platforms such as Facebook, Instagram, and Twitter further suggests that we can not ignore such multimodal content (vision and language) anymore. To the best of our knowledge, there is not much attention towards meme emotion, and sentiment analysis. <br/>
The task of Memotion analysis consists of three task (1) sentiment(positive, negative, neutral) classification, (2) emotion (sarcastic,funny,offensive, motivation) classification and (3) their corresponding intensity. <br/></p>
<p>Here is some example of Meme Images, they contain two information simultaneously which is text and image: <br/></p>
<p align="center">
  <img src="/blog/img/challenge/meme_example.png">
</p>
<h3 id="2-model">2. Model</h3>
<p>The models consist of 3 primary types which are Unimodal for Only Text, Unimodal for Only Image, Multimodal for Text and Image.<br/>
In the Multimodal, I also employed several fusion mechanism such as Concatenation, Multihop Attention, and Stacked Attention. <br/></p>
<h4 id="21-unimodal-for-text">2.1 Unimodal for Text</h4>
<p>BERT and its variant, e.g., RoBERTa, are widely used in Natural Language Processing (NLP) tasks and have demonstrated as efficient methods. In this competition, we employed them for three subtasks. In subtasks A and B, a RoBERTa is used while in subtask C, four RoBERTa models are adopted so that every backbone corresponds classifying the intensity of each emotion.</p>
<h4 id="22-unimodal-for-image">2.2 Unimodal for Image</h4>
<p>As a vision-based approach, EfficientNet-v2 is a well-known backbone with respect to speedy inference and a low number of parameters. In three subtasks, EfficientNet-v2 is used as an extractor to create embeddings. Subtask A has one classification branch to deal with three sentiment types while subtasks B and C have four branches which each is responsible for classifying four types of emotions as well as their intensity.</p>
<h4 id="23-multimodal">2.3 Multimodal</h4>
<p>Multi-modality is to aggregate vision and text to obtain correlated information. In this challenge, we build three different fusion models including concatenation, multi-hop attention, and stacked attention network.</p>
<h5 id="231-concatenation">2.3.1 Concatenation</h5>
<p>Traditionally, concatenation of two feature vectors, a.k.a two modalities, has been a typical solution to obtain aggregated features. However, the method does not take into account the importance of each word that is within corresponding regions of the image.</p>
<h5 id="232-multi-hop-attention">2.3.2 Multi-hop Attention</h5>
<p>Multi-hop attention focuses parts of a given image together with texts within it. The technique aims to emphasize dissimilar features between image regions and textual utterances by defining a relevant matrix $R$, which is the cosine distance between textual and visual features.</p>
<h5 id="233-stacked-attention">2.3.3 Stacked Attention</h5>
<p>While a multi-hop attention network is used to learn attention maps between an image and texts within it, a stacked attention network has a capability of learning an attention map in multiple times. Through such attention layers, interested regions are promoted through a referred concept within a given sentence.</p>
<h3 id="3-techniques">3. Techniques</h3>
<p>In this competition, I have used some techniques to improve the F1 scores of the model.</p>
<h4 id="31-scence-text-detection">3.1 Scence Text Detection</h4>
<p>Although both textual and visual features are important for meme emotion analysis, there is little correlation between them in the MEMOTION 2.0 dataset. Besides, the caption is also provided as a part of the dataset. Therefore, in this study, the text is removed from the image before extracting and training the proposed model.</p>
<p>Based on the previous works that summarized both traditional and deep learning approaches for text detection and recognition, we design a preprocessing scheme to remove texts from images as follows. First, we employ the EAST module to detect all text regions in an image. Then these regions are removed from the image, and we use the output image as the input for EfficientNet-v2 in the proposed framework.</p>
<p align="center">
  <img src="/blog/img/challenge/meme_preproc.png">
</p>
Preprocessing scheme: Given an image as input, we use the EAST detector to detect the region of texts on the image and then remove them
<h4 id="32-auto-augmentation">3.2 Auto Augmentation</h4>
<p>Augmentation is a simple but important technique to increase the size of a given dataset, leading to a better generalization of a training model. However, current data augmentation is based on a set of manually designed algorithms such as Crop, Rotation, and Resize. In our experiment, we adopt the Auto Augment technique which uses reinforcement learning to automatically search for a better data augmentation strategy.</p>
<h4 id="33-canonical-correlation-analysis">3.3 Canonical Correlation Analysis</h4>
<p>The Canonical correlation analysis (CCA) is based on a well-established statistical technique that searches for a linear combination of input vectors by maximizing their correlations. Deep CCA tries to utilize the power of both deep neural networks and CCA to overcome projection constraints of CCA . In this study, correlation scores obtained from Deep CCA is included to our loss function to maximize the correlation between two features, leading to a higher classification rate.</p>
<p align="center">
  <img src="/blog/img/challenge/cca.png">
</p>
<h4 id="34-loss-function-optimizer-etc">3.4 Loss function, Optimizer, etc</h4>
<p>For the classification task, I use CrossEntropy typically, but I also tested on Training Signal Anealing CrossEntropy which is a loss function that helps mitigate the imbalance effect of the dataset. <br/>
In terms of Optimizer, I tried several ones such as Adam, SGD, AdamW, LookAHead, etc.</p>
<h3 id="4-result">4. Result</h3>
<p align="center">
  <img src="/blog/img/challenge/meme_result.png">
</p>
Some note about this competition: This is not a fair competition because after the private phase, they recruited more teams to submit and manipulated the results several times. This is quite a pity because I didn't notice that all the chair is Indian....
<h3 id="5-application">5. Application</h3>
<p>Analysis meme images can help social media platforms such as Facebook, Twitter mitigate the false information that are disemminating widely.<br/>
Moreover, the technique in this post can be applied to other Vision Language problems.</p>
<h3 id="6-code-and-paper">6. Code and Paper</h3>
<p>Link code: <a href="https://github.com/ngthanhtin/Memotion2_AAAI_WS_2022">Code</a> <br/>
Link paper: Updating&hellip;<a href="https://github.com/ngthanhtin/Memotion2_AAAI_WS_2022">Paper</a></p>
<div style="text-align: right"> (Tín Nguyễn) </div></article>
    </section>

    

    
    <article class="ui segment utterances-comments" data-html2canvas-ignore>
      <script src="https://utteranc.es/client.js"
        repo="ngthanhtin/blog"
        issue-term="pathname"
        label="comment"
        theme="github-light"
        crossorigin="anonymous"
        async>
      </script>
    </article>
    

    
  </div>
</div>


            <footer class="ui basic center aligned segment" style="background-color: transparent;">
              
              <p>© 2021 - 2022 Tin&#39;s Blog</p>
              

              <p>Powered by <a href="https://gohugo.io/" target="_blank">Hugo</a> with theme <a href="https://github.com/g1eny0ung/hugo-theme-dream" target="_blank">Dream</a>.</p>

              
            </footer>
          </div>
        </section>
        <section class="back">
          <div class="dream-max-width">
            <header class="ui basic very padded segment dream-header">
  <div class="ui small circular image">
    
    <img src="/blog/img/avatar.png" />
    
  </div>

  <div class="content">
    <h1 class="ui medium header">Nguyen Thanh Tin&#39;s blog<div class="sub header">luffy</div>
    </h1>

    <article class="ui horizontal list">
      
      <a class="item" href="/blog/categories">
        <i class="th list icon" title="All Categories"></i>
      </a>
      <a class="item" href="/blog/tags">
        <i class="tags icon" title="All Tags"></i>
      </a>
    </article>

    
    <article class="dream-tags">
    
      
      
      <a class="ui label" href="/blog/tags/donate/" title="Donate">Donate</a>
    
      
      
      <a class="ui label" href="/blog/tags/korea/" title="Korea">Korea</a>
    
      
      
      <a class="ui label" href="/blog/tags/medical/" title="Medical">Medical</a>
    
      
      
      <a class="ui label" href="/blog/tags/ml-technique/" title="Ml Technique">Ml Technique</a>
    
      
      
      <a class="ui label" href="/blog/tags/multiple-object-tracking/" title="Multiple Object Tracking">Multiple Object Tracking</a>
    
      
      
      <a class="ui label" href="/blog/tags/optimization-math/" title="Optimization Math">Optimization Math</a>
    
      
      
      <a class="ui label" href="/blog/tags/personal/" title="Personal">Personal</a>
    
      
      
      <a class="ui label" href="/blog/tags/reinforcement-learning/" title="Reinforcement Learning">Reinforcement Learning</a>
    
      
      
      <a class="ui label" href="/blog/tags/scholarship/" title="Scholarship">Scholarship</a>
    
      
      
      <a class="ui label" href="/blog/tags/single-object-tracking/" title="Single Object Tracking">Single Object Tracking</a>
    
      
      
      <a class="ui label" href="/blog/tags/slam/" title="Slam">Slam</a>
    
      
      
      <a class="ui label" href="/blog/tags/software-tips/" title="Software Tips">Software Tips</a>
    
      
      
      <a class="ui label" href="/blog/tags/vision-language/" title="Vision Language">Vision Language</a>
    
      
      
      <a class="ui label" href="/blog/tags/winter/" title="Winter">Winter</a>
    
    </article>
    
  </div>
</header>


<div class="ui relaxed grid dream-grid dream-back">
  
  
  

  <div class="sixteen wide mobile eight wide tablet four wide computer column dream-column">
    <article class="ui segment">
      <h3 class="ui header">Social Links</h3>
      <nav class="ui secondary menu dream-menu dream-socials">
  

  
  <div class="item">
    <a href="mailto:ngthanhtinqn@gmail.com">
      <i class="large mail icon" title="Email"></i>
    </a>
  </div>
  

  

  

  

  

  

  

  

  
</nav>

    </article>
  </div>

  <div class="sixteen wide mobile eight wide tablet four wide computer column dream-column">
    
  </div>

  
  
  
</div>

          </div>
        </section>
      </div>
    </div>

    <script>
  window.defaultDark =  null 
  window.backgroundDark = "black"
  window.backgroundImageDark =  null 
  window.darkNav =  null 
  window.hasTwitterEmbed =  null 

  if (window.hasTwitterEmbed) {
    
    window.twttr = (function (d, s, id) {
      var js,
        fjs = d.getElementsByTagName(s)[0],
        t = window.twttr || {}
      if (d.getElementById(id)) return t
      js = d.createElement(s)
      js.id = id
      js.src = 'https://platform.twitter.com/widgets.js'
      fjs.parentNode.insertBefore(js, fjs)

      t._e = []
      t.ready = function (f) {
        t._e.push(f)
      }

      return t
    })(document, 'script', 'twitter-wjs')
  }
</script>
<script src="/blog/js/jquery.min.js"></script>
<script src="/blog/js/semantic.min.js"></script>
<script src="/blog/js/jquery.overlayScrollbars.min.js"></script>
<script src="/blog/js/header.js"></script>
<script src="/blog/js/main.js"></script>
<script src="/blog/js/theme.js"></script>

    
<script src="/blog/js/html2canvas.min.js"></script>
<script src="/blog/js/post.js"></script>





    

    

    
  </body>
</html>
