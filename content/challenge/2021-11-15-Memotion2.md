---
layout: post
title: Understanding Sentiment, Emotion and its Intensity by Analysing Meme Images
subtitle: Vision and Language
date : "2021-11-15"
tags: "Vision-Language"
categories: "Vision-Language"
comments: true

cover: "/blog/img/challenge/meme.jpg"
---
In this post, I want to talk about Meme Analysis which is about explaining its Sentiment, Emotion and Intensity. <br/>
The growing ubiquity of Internet memes on social media platforms such as Facebook, Instagram, and Twitter further suggests that we can not ignore such multimodal content (vision and language) anymore. To the best of our knowledge, there is not much attention towards meme emotion, and sentiment analysis. <br/>
The task of Memotion analysis consists of three task (1) sentiment(positive, negative, neutral) classification, (2) emotion (sarcastic,funny,offensive, motivation) classification and (3) their corresponding intensity. <br/>

Here is some example of Meme Images, they contain two information simultaneously which is text and image: <br/>

<p align="center">
  <img src="/blog/img/challenge/meme_example.png">
</p>

Link code: [Code](https://github.com/ngthanhtin/Memotion2_AAAI_WS_2022)

<section id="1. Introduction to Instruction Navigation">
<b>1. Introduction to Instruction Navigation</b>
</section>
Navigation problem is a familiar topic and it has been carried out many experiments in so many years. The problem is to make an agent move in an environment and achieve a predefined target. In general, this problem can be solved by Reinforcement Learning, the agent will receive an observation which contains several information such as image, depth, segmentation or sensor information, etc. Howerver, human want to make command for the agent through instruction, so the agent now has an additional information which is a text, or voice. The low-level instruction will be simple commands such as go straight, go left, go right, etc, but in practice, the instruction is more complex, it needs to consider other elements such as size, color, etc of the objects nearby. And that is the motivation of the Instruction Navigation problem.
<p align="center">
  <img src="/blog/img/instruction_navigation/instruction_robot_navigation.gif">
</p>

<section id="2. Principle">
<b>2. Principle</b>
</section>
The principle of Vision-Language problems is how to fuse multiple features together. <br/>
And with this problem, it composes of 3 components, (1) is to get image features, (2) is to get textual features, (3) is how to fuse these features together which is the most important part. In this paper, the fusion strategy they've used is concatenation and Hadarmard (element-wise) product. The final stage is the decision-making part, which employs a policy learning algorithm to make decision. A well-known but simple algorithm used in this situation is Asynchronous Actor Critic (A3C).

<section id="3. Methodology">
<b>3. Methodology</b>
</section>
<p align="center">
  <img src="/blog/img/instruction_navigation/pp.png">
</p>
The methodology simply takes an image and an instruction as inputs. Noted that, with each episode, there is only one instruction, but the image changes continually.

* <b>3.1 Image Representation Module</b><br/>
In order to extract visual features, they employed a simple 3-layer CNN.

* <b>3.2 Text Representation Module</b><br/>
In terms of extracting textual features, first, they employed Word Embedding on the text input, then converted it to sentence embedding by using LSTM or GRU.

* <b>3.3 Attented Representation Module</b><br/>
<p align="center">
  <img src="/blog/img/instruction_navigation/attention.png">
</p>
After getting a hold of 2 feature vectors, they use Hadardmard (element-wise) product to fuse the features together, and this is called Gated Attention.

* <b>3.4 Policy Learning</b><br/>
<p align="center">
  <img src="/blog/img/instruction_navigation/policy.png">
</p>
The policy learning will help the agent make decision which direction is going on. In this paper, the author use A3C, which is a well-known but simple reinforcement learning algorithm.

<section id="4. Application">
<b>5. Application</b>
</section>
This algorithm can be applied to robotics, however, this stll have some limits such as only handling short, and simple instructions.


<section id="5. Reference">
<b>6. Reference</b>
</section>

[Paper](https://arxiv.org/abs/1706.07230)<br/>
[Code](https://github.com/devendrachaplot/DeepRL-Grounding)

<div style="text-align: right"> (Tín Nguyễn) </div>