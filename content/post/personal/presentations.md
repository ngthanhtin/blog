---
title : "Presentations"
date : "2024-07-14"
tags : [personal, idea, presentation, talk]
thumbnail : "/images/personal/presentation.png"
series:
  - Personal
---
{{% notice tip "Idea" %}}
2024
<li>
    <a href="https://docs.google.com/presentation/d/18VRhnW-AqH5BIaLsbg5Fb8J8k-J07SkQ-8NVhIDH8F8/edit?usp=sharing">[2024] Idea Proposal</a>
</li>
{{% /notice %}}

{{% notice note "Lab Meeting" %}}
2024
<li>
<a href="https://docs.google.com/presentation/d/1yXs90ZEulgudqJ8iwRwELD6rXPHMTKManH6hmxIqCpE/edit?usp=sharing">[Sep 10, 2024] Faith and Fate: Transformers as fuzzy pattern matchers</a>
</li>
<li>
<a href="https://docs.google.com/presentation/d/17J3HIyS8E9i5LReGhr612rqYwwlZWYUJzcFEdF9pOjo/edit?usp=sharing">[Aug 13, 2024] Faithful CoT</a>
</li>
<li>
<a href="https://docs.google.com/presentation/d/17J3HIyS8E9i5LReGhr612rqYwwlZWYUJzcFEdF9pOjo/edit?usp=sharing">[Jul 22, 2024] The Probabilities Also Matter: A More Faithful Metric for Faithfulness of Free-Text Explanations in LLMs</a>
</li>
<li>
<a href="https://docs.google.com/presentation/d/1N3mWA0g4eUx03w9BFN42J9m13IrGunuqfUG-hRH-AbI/edit?usp=sharing">[Jul 10, 2024] Why are Visually-Grounded Language Models Bad at Image Classification</a>
</li>
<li>
    <a href="https://docs.google.com/presentation/d/1LS9GMw-jvzlryrGhFyrYUUNVZLIs39wR9bWtgqnsL-4/edit?usp=sharing">[May 16, 2024] Hierarchical Open-vocabulary Universal Image Segmentation</a>
</li>
			
<li>
    <a href="https://docs.google.com/presentation/d/1HRSanGtjoqwYbqGUSH3oZUnhn1vpyVkKPMMP_HQ4J_Q/edit?usp=sharing">[Apr 16, 2024] GLaMM: Pixel Grounding Large Multimodal Model</a>
</li>
<li>
    <a href="https://docs.google.com/presentation/d/1-SRQz4CqDnZ5-kKF3_ToXjKqAZZikTwQAFLWMZifaAk/edit?usp=sharing">[Mar 12, 2024] Improved Zero-shot Classification by Adapting VLMs with Text Descriptions</a>
</li>
<li>
    <a href="https://docs.google.com/presentation/d/1JjP8kFnOxK90CkCIlKZaozbL9Bb4LG94TwQPelYyEX0/edit?usp=sharing">[Jan 30, 2024] Classification based on Boxes and Phrases </a><b>(Research Project)</b>
</li>

2023			
<li>
    <a href="https://docs.google.com/presentation/d/16E1Be4lKNtT9u0kqQJUdxK-7IPKNbuL17PsRlkj7q94/edit?usp=sharing">[Nov 21, 2023] What does CLIP know about a red circle? Visual prompt engineering for VLMs</a>
</li>
<li>
    <a href="https://docs.google.com/presentation/d/10fMrm3c-EnHM_k98__NnK5e_lemufEmir5LoGp1s4a0/edit?usp=sharing">[Oct 3, 2023] CLIP-Event: Connecting Text and Images with Event Structures</a>
</li>
<li>
    <a href="https://docs.google.com/presentation/d/1JF96OjDiElYhzWU7Uefhnulykd4X1_NNfYtgDUC5i2w/edit?usp=sharing">[Aug 25, 2023] How does “habitat” help fine-grained bird identification? </a><b>(Research Project)</b>
</li>
<li>
    <a href="https://docs.google.com/presentation/d/1vrFCwyjRfT2z575tpbE9AjNppzu_Psa4pykWJEu2NKg/edit?usp=sharing">[Apr 25, 2023] Zero-Shot Classification by Logical Reasoning on Natural Language Explanations</a>
</li>
<li>
    <a href="https://docs.google.com/presentation/d/1w0haDsy9RsR0UqmDS2vJKTm-ot7PPmxpYJhPGx64lmc/edit?usp=sharing">[Mar 21, 2023] Open-Vocabulary Semantic Segmentation With Mask-Adapted CLIP </a>
</li>
<li>
    <a href="https://docs.google.com/presentation/d/1L30yCbTVfkphIicOU0JYjBhu5z_4Me0cjcLSr2LOcu4/edit#slide=id.g20703a8677a_0_78">[Feb 14, 2023] STAIR: Learning Sparse Text and Image Representation in Grounded Tokens</a>
</li>

2022
<li>
    <a href="https://docs.google.com/presentation/d/1nW0lQZXS1ERXmSGtzEyBcLosnw8OsdQCGM87H1vHels/edit?usp=sharing">[Nov 22, 2022] Re-labeling ImageNet - from Single to Multi-Labels, from Global to Localized Labels</a>
</li>
<li>
<a href="https://docs.google.com/presentation/d/11LSAUoDC0QSogBQZrHWoM7Zc3ocMQdHc_FC8339FB2M/edit#slide=id.p">[Oct 18, 2022] Class Activation Latent Mapping - Keep CALM and Improve Visual Feature Attribution</a>
</li>
<li>
    <a href="https://docs.google.com/presentation/d/13khKc8L5HbRJWe6wKKxpLvRkpzIPc5I1ejXi9dOgnFs/edit?usp=sharing">[Sep 20, 2022] Mask2Former: Masked-attention Mask Transformer for Universal Image Segmentation</a> </br>
</li>

{{% /notice %}}